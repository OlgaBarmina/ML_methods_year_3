{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQlcDy9BC5bE"
      },
      "source": [
        "# Методы машинного обучения – Лабораторная работа №3\n",
        "\n",
        "# Нелинейная регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKlRxa2EKcTu"
      },
      "source": [
        "Импортируем необходимые библиотеки:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tfds-nightly"
      ],
      "metadata": {
        "id": "kJ3N3voqJRAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCNmPhYuurZN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea4FLN13u0ER"
      },
      "source": [
        "### Моделирование градиентного спуска\n",
        "\n",
        "Градиентный спуск — метод нахождения локального минимума или максимума функции с помощью движения вдоль градиента. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB8EvIk5vDFY"
      },
      "outputs": [],
      "source": [
        "plot_x = np.linspace(-1., 6., 141)\n",
        "plot_y = (plot_x-2.5)**2 - 1.\n",
        "plt.plot(plot_x, plot_y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBM-bSvsDZgM"
      },
      "source": [
        "Для вычисления значений и производной функции $y = (x-2.5)^2-1$ будем использовать следующие функции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTwVnLapvLbP"
      },
      "outputs": [],
      "source": [
        "def J(x_):\n",
        "    return (x_-2.5)**2 - 1.\n",
        "\n",
        "def dJ(x_):\n",
        "    return 2*(x_-2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfbCnAOmvo_n"
      },
      "source": [
        "Для моделирования градиентного спуска будем использовать функцию `gradient_descent`, которая будет запоминать и возвращать историю итераций, для визуализации градиентного спуска будем использовать функцию `plot_history`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7APqhK-vz6X"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(initial_x, eta, n_iters = 1e4, epsilon=1e-8):\n",
        "    x_ = initial_x\n",
        "    x_history = [initial_x]\n",
        "    i_iter = 0\n",
        "\n",
        "    while i_iter < n_iters:\n",
        "        gradient = dJ(x_)\n",
        "        last_x_ = x_\n",
        "        x_ -= eta * gradient\n",
        "        x_history.append(x_)\n",
        "    \n",
        "        if(abs(J(x_) - J(last_x_)) < epsilon):\n",
        "            break\n",
        "        i_iter += 1\n",
        "        \n",
        "    return x_history\n",
        "            \n",
        "def plot_history(plot_x, x_history):\n",
        "    plt.plot(plot_x, J(plot_x))\n",
        "    plt.plot(np.array(x_history), J(np.array(x_history)), color=\"r\", marker='+')\n",
        "    plt.text(1., 10., f'Кол-во шагов: {len(x_history)}', fontsize=14, color='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09jU4noIwExK"
      },
      "source": [
        "Проведем моделирование градиентного спуска с различным шагом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjLFRgqhwY1H"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.1)\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfnTTBoODZgO"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.01)\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRZ_GnGTDZgO"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.001)\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FmWe2CXDZgP"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.8)\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95uN08DfCWtf"
      },
      "source": [
        "При дальнейшем увеличении шага возникает ошибка, которую нужно обработать:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxxozfeOC4gL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    hist = gradient_descent(0., 1.1)\n",
        "    plot_history(plot_x, hist)\n",
        "except Exception as e: \n",
        "    print(f\"{type(e).__name__}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hwtgwQUDzNP"
      },
      "source": [
        "Ограничимся десятью итерациями:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9tt2CDCDZgQ"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 1.1, n_iters=10)\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTgbNgA1TSFd"
      },
      "source": [
        "### Стохастический градиентный спуск\n",
        "\n",
        "Стохастический градиентный спуск (stochastic gradient descent, SGD) − оптимизационный алгоритм, отличающийся от обычного градиентного спуска тем, что градиент оптимизируемой функции считается на каждом шаге не как сумма градиентов от каждого элемента выборки, а как градиент от одного, случайно выбранного элемента или некоторой подвыборки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL49HDihTSFe"
      },
      "outputs": [],
      "source": [
        "m = 100000\n",
        "\n",
        "x = np.random.normal(size=m)\n",
        "X = x.reshape(-1,1)           # преобразуем вектор в матрицу с одним столбцом\n",
        "y = 4.*x + 3. + np.random.normal(0, 3, size=m)\n",
        "\n",
        "plt.scatter(x, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxvjPYbGTSFf"
      },
      "source": [
        "Определим класс `RegressionSGD`, использующий стохастический градиентный спуск (с переменным шагом) при обучении модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YPqe8uADZgS"
      },
      "outputs": [],
      "source": [
        "class RegressionSGD:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self._theta = None\n",
        "\n",
        "    def fit(self, X_train, y_train, n_iters=50, t0=5, t1=50):\n",
        "        assert X_train.shape[0] == y_train.shape[0], \\\n",
        "            \"Размер X_train должен быть равен размеру y_train\"\n",
        "        assert n_iters >= 1\n",
        "\n",
        "        def dJ_sgd(theta, X_b_i, y_i):\n",
        "            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.\n",
        "\n",
        "        def sgd(X_b, y, initial_theta, n_iters=5, t0=5, t1=50):\n",
        "\n",
        "            def learning_rate(t):\n",
        "                return t0 / (t + t1)\n",
        "\n",
        "            theta = initial_theta\n",
        "            m = len(X_b)\n",
        "            for i_iter in range(n_iters):\n",
        "                indexes = np.random.permutation(m)\n",
        "                X_b_new = X_b[indexes,:]\n",
        "                y_new = y[indexes]\n",
        "                for i in range(m):\n",
        "                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])\n",
        "                    theta = theta - learning_rate(i_iter * m + i) * gradient\n",
        "\n",
        "            return theta\n",
        "\n",
        "        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n",
        "        initial_theta = np.random.randn(X_b.shape[1])\n",
        "        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)\n",
        "\n",
        "        self.intercept_ = self._theta[0]\n",
        "        self.coef_ = self._theta[1:]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_predict):\n",
        "        assert self.intercept_ is not None and self.coef_ is not None, \\\n",
        "            \"Нужно обучить модель перед использованием!\"\n",
        "        assert X_predict.shape[1] == len(self.coef_), \\\n",
        "            \"Кол-во признаков в X_predict должно быть равно кол-ву признаков в X_train\"\n",
        "\n",
        "        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])\n",
        "        return X_b.dot(self._theta)\n",
        "\n",
        "    def score(self, X_test, y_test):\n",
        "        y_predict = self.predict(X_test)\n",
        "        return r2_score(y_test, y_predict)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"RegressionSGD()\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdWhiVf9DZgS"
      },
      "source": [
        "Используем созданный класс для обучения модели регрессии на сгенерированных ранее данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwcjZJSNDZgS"
      },
      "outputs": [],
      "source": [
        "reg = RegressionSGD()\n",
        "reg.fit(X, y, n_iters=2)\n",
        "reg.coef_, reg.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmnuKXhFDZgT"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plot_x = np.linspace(-4, 4, 101)\n",
        "plt.plot(plot_x, reg.predict(plot_x.reshape(-1,1)), c='r', lw=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FWTQ7uOeVNS"
      },
      "source": [
        "### Полиномиальная регрессия\n",
        "\n",
        "Полиномиальная регрессия может использоваться для решения задачи регрессии для нелинейных данных. В полиномиальной регрессии используется кривая линия, соответствующая полиному степени больше, чем 1. Например, пусть входные данные соответствуют зависимости $y=0.5 x^2+x+2$ (с нормальным шумом):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3a8MHelDZgU"
      },
      "outputs": [],
      "source": [
        "x = np.random.uniform(-3, 3, size=100) # вектор\n",
        "X = x.reshape(-1, 1)                   # матрица с одним стобцом \n",
        "y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)\n",
        "\n",
        "plt.scatter(x, y);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGCeZCQyev5U"
      },
      "outputs": [],
      "source": [
        "reg = RegressionSGD()\n",
        "reg.fit(X, y, n_iters=2)\n",
        "y_predict = reg.predict(X)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, y_predict, color='r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj3XKkYGDZgU"
      },
      "source": [
        "Подготовим для модели регрессии входные данные с двумя признаками – линейной и квадратичной зависимостью от независимой переменной:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uwr9YMgDZgU"
      },
      "outputs": [],
      "source": [
        "X2 = np.hstack([X, X**2]) # соединение массивов по горизонтали\n",
        "X2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valt5SpDDZgV"
      },
      "source": [
        "Применим к построенным данным модель регрессии на основе SGD и нарисуем набор данных и линию регрессии (функция `np.argsort` возвращает индексы элементов в отсортированном массиве):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB4SM5yie75R"
      },
      "outputs": [],
      "source": [
        "reg2 = RegressionSGD()\n",
        "reg2.fit(X2, y, n_iters=2000)\n",
        "y_predict2 = reg2.predict(X2)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(np.sort(x), y_predict2[np.argsort(x)], c='r', lw=3); "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Km8H1mDZgW"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y_predict2, c='r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwpOQ-Y8DZgX"
      },
      "source": [
        "Определенные коэффициенты регрессии и смещение близки к значениям, использованным при генерации данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_vqJAHwDZgX"
      },
      "outputs": [],
      "source": [
        "reg2.coef_, reg2.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Yls-NBDZgY"
      },
      "source": [
        "### Полиномиальная регрессия при помощи TensorFlow\n",
        "\n",
        "Полиномиальная регрессия также может быть реализована при помощи TensorFlow. Создадим простейшую нейронную сеть с одним слоем из одного нейрона и двумя входными нейронами:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjxGTjM2DZgY"
      },
      "outputs": [],
      "source": [
        "reg2_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(2,)),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcE7nTWfDZgZ"
      },
      "source": [
        "В такой нейронной сети будет всего три обучаемых параметра:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdsk3LDIDZgZ"
      },
      "outputs": [],
      "source": [
        "reg2_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzfVPvb-DZgZ"
      },
      "source": [
        "При компиляции модели в обязательном порядке указывается функция потерь (`loss`), также может быть выбран оптимизатор с параметрами (`optimizer`), метрики для оценки обучения (`metrics`) и некоторые другие другие параметры. В качестве функции потерь и/или метрики могут быть, в частности,  выбраны среднеквадратичная ошибка (MSE) и средняя абсолютная ошибка (MAE). Коэффициента детерминации $R^2$ среди стандартных функций потерь и метрик нет, но он легко может быть вычислен непосредственно по показателю MSE и общей дисперсии целевой переменной."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MszTzK1ADZga"
      },
      "outputs": [],
      "source": [
        "reg2_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojv5wSvCDZga"
      },
      "source": [
        "Обучаем нейронную сеть на полиномиальных зависимостях:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OgI6BUPDZgb"
      },
      "outputs": [],
      "source": [
        "history = reg2_model.fit(\n",
        "    X2, y, \n",
        "    epochs=100,\n",
        "    # уровень выводимой информации\n",
        "    verbose=1,\n",
        "    # проверка (валидация) на 20% обучающих данных\n",
        "    validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmx84U1TDZgb"
      },
      "source": [
        "Метод `fit` возвращает объект `history`, в котором для задачи регрессии обычно есть ключи `'loss'` и `'val_loss'`. Можно визуализировать историю обучения при помощи следующей функции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UC0XgsXDZgb"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.ylim([0, max(history.history['loss'])*0.5])\n",
        "  plt.title('Функция потерь при обучении модели')\n",
        "  plt.xlabel('Эпохи обучения')\n",
        "  plt.ylabel('Функция потерь')\n",
        "  plt.legend(['Обучающая выборка', 'Тестовая выборка'], loc='upper right')\n",
        "  plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIyqz6QyDZgb"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4jZTEVXDZgc"
      },
      "source": [
        "При помощи обученной модели можно выполнить прогноз:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv6sIl2WDZgc"
      },
      "outputs": [],
      "source": [
        "y_predict_reg2 = reg2_model.predict(X2)\n",
        "\n",
        "plt.scatter(x, y, label='набор данных')\n",
        "plt.plot(np.sort(x), y_predict_reg2[np.argsort(x)], color='r', label='прогноз')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tls9AAeADZgc"
      },
      "source": [
        "### Нелинейная парная регрессия при помощи TensorFlow\n",
        "\n",
        "Универсальная теорема аппроксимации утверждает, что любую непрерывную функцию можно с любой степенью точности аппроксимировать нейронной сетью с одним скрытым слоем с функцией активации сигмоида.\n",
        "\n",
        "Построим и обучим нейронную сеть такого типа для рассматриваемого набора данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0zfUq97DZgd"
      },
      "outputs": [],
      "source": [
        "uni_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,)),\n",
        "    tf.keras.layers.Dense(units=512, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37PfKKTgDZgd"
      },
      "outputs": [],
      "source": [
        "uni_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqmlqC4vDZgd"
      },
      "outputs": [],
      "source": [
        "uni_model.compile(loss='mse', \n",
        "                  optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "                  metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzfTuPODDZge"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP-6t-RFDZge"
      },
      "outputs": [],
      "source": [
        "history = uni_model.fit(\n",
        "    X, y, \n",
        "    epochs=100,\n",
        "    # уровень выводимой информации\n",
        "    verbose=1,\n",
        "    # проверка (валидация) на 20% обучающих данных\n",
        "    validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eExznGLgDZge"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj3CCBj_DZgf"
      },
      "outputs": [],
      "source": [
        "y_predict_uni = uni_model.predict(X)\n",
        "\n",
        "plt.scatter(x, y, label='набор данных')\n",
        "plt.plot(np.sort(x), y_predict_uni[np.argsort(x)], color='r', label='прогноз')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0TMN5AfDZgf"
      },
      "source": [
        "### Нелинейная множественная регрессия при помощи TensorFlow\n",
        "\n",
        "При помощи нейронных сетей с нелинейными функциями активации нейронов можно успешно решать задачи нелинейной регрессии.\n",
        "\n",
        "Загрузим из TesorFlow Datasets набор `howell` с демографическими данными жителей Калахари:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmpsafoIDZgf"
      },
      "outputs": [],
      "source": [
        "ds = tfds.load(\"howell\", split='train')\n",
        "df = tfds.as_dataframe(ds)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68gJZ7OdDZgg"
      },
      "source": [
        "Изучим зависимость возраста от роста и веса:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M9GDctEDZgg"
      },
      "outputs": [],
      "source": [
        "X = np.array(df[['height','weight']])\n",
        "y = np.array(df[['age']]).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDkd6x5KDZgg"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR3-cA-JDZgg"
      },
      "source": [
        "### Визуализация трехмерных данных\n",
        "\n",
        "Для построения 3d графиков необходимо импортировать необходимые модули:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkh9NxlTDZgh"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "# или from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw7aoJezDZgh"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d') # или ax = fig.add_subplot(111, projection='3d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmpl1twGDZgh"
      },
      "source": [
        "Для построения точечного графика можно использовать функцию `scatter()`, которой передаются три параметра для координат точек по осям X, Y и Z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I9dEFn2DZgh"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12,10)) \n",
        "ax = plt.axes(projection='3d') \n",
        "\n",
        "xs = X[:,0]\n",
        "ys = X[:,1]\n",
        "zs = y\n",
        "\n",
        "ax.scatter( xs, ys, zs, s=100 ) \n",
        "ax.set_xlabel('Рост') \n",
        "ax.set_ylabel('Вес') \n",
        "ax.set_zlabel('Возраст') \n",
        "ax.view_init( azim=-120, elev=25 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x8LgJ79DZgi"
      },
      "source": [
        "### Глубокая нейронная сеть для задачи регрессии\n",
        "\n",
        "Используем слой нормализации, адаптированный к обоим независимым признакам:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRfe-FwrDZgi"
      },
      "outputs": [],
      "source": [
        "feature_normalizer = tf.keras.layers.Normalization(axis=None,input_shape=(2,)) \n",
        "feature_normalizer.adapt(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPrecAIXDZgi"
      },
      "source": [
        "Создадим нейронную сеть со слоем нормализации, четырьмя скрытыми плотными слоями с 64 нейронами и функцией активации ReLu и выходным слоем из одного нейрона:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prq4NmL_DZgi"
      },
      "outputs": [],
      "source": [
        "large_model = tf.keras.Sequential([\n",
        "    feature_normalizer,\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "large_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QDL89j2DZgj"
      },
      "source": [
        "Скомпилируем модель, используя в качестве функции потерь  среднеквадратичную ошибку MSE с оптимизатором по умолчанию (RmsProp):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X09hWPb_DZgj"
      },
      "outputs": [],
      "source": [
        "large_model.compile(loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV3HWS63DZgj"
      },
      "source": [
        "Обучим модель на наборе данных из двух признаков:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M76pdinIDZgk"
      },
      "outputs": [],
      "source": [
        "history = large_model.fit(\n",
        "    X, y, \n",
        "    epochs=100,\n",
        "    # уровень выводимой информации\n",
        "    verbose=1,\n",
        "    # проверка (валидация) на 30% обучающих данных\n",
        "    validation_split = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWONsB5PDZgk"
      },
      "source": [
        "Кривые обучения в зависимости от эпохи обучения выглядят так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b7fAwNeDZgk"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re5YNOwGDZgl"
      },
      "source": [
        "Для визуализации прогнозируемых множественной регрессией значений воспользуемся функцией `plot_surface`. Потребуются определенные усилия по подготовке данных для `plot_surface`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2u-i6o1DZgl"
      },
      "outputs": [],
      "source": [
        "n_plot = 51\n",
        "\n",
        "x_plot = np.linspace(np.min(xs), np.max(xs), n_plot) \n",
        "y_plot = np.linspace(np.min(ys), np.max(ys), n_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej8c7y4NDZgl"
      },
      "outputs": [],
      "source": [
        "x_mesh, y_mesh = np.meshgrid(x_plot, y_plot)\n",
        "x_mesh.shape, y_mesh.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUAGCnYnDZgl"
      },
      "outputs": [],
      "source": [
        "x_plot2 = np.reshape(x_mesh, [n_plot**2,1])\n",
        "y_plot2 = np.reshape(y_mesh, [n_plot**2,1])\n",
        "xy_2 = np.hstack([x_plot2, y_plot2])\n",
        "xy_2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2BPrTSKDZgm"
      },
      "source": [
        "Теперь выполним прогнозирование при помощи обученной ранее модели, после чего вернемся к форме 51 на 51:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULXbAwORDZgm"
      },
      "outputs": [],
      "source": [
        "z = large_model.predict(xy_2)\n",
        "z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A53ujobhDZgm"
      },
      "outputs": [],
      "source": [
        "z_mesh = z.reshape((n_plot, n_plot))\n",
        "z_mesh.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-7r4Z4UDZgm"
      },
      "source": [
        "Функция `plot_surface` имеет большое число параметров, в частности:\n",
        "\n",
        "* X, Y, Z : 2D массивы – данные для построения поверхности.\n",
        "* rstride, cstride : int – параметры определяют величину шага, с которым будут браться элементы строки/столбца из переданных массивов.\n",
        "* cmap: Colormap – цветовая карта для элементов поверхности.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXJR1T-0DZgn"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "surf = ax.plot_surface(x_mesh, y_mesh, z_mesh, \\\n",
        "       rstride=1, cstride=1, linewidth=0.05, cmap=cm.winter, antialiased=True, \\\n",
        "       edgecolors='gray') \n",
        "ax.scatter( xs, ys, zs, s=100, c='r' )\n",
        "\n",
        "ax.set_xlabel('Рост', fontsize=14) \n",
        "ax.set_ylabel('Вес', fontsize=14)\n",
        "ax.set_zlabel('Возраст', fontsize=14) \n",
        "ax.set_title('Демографические данные обитателей Калахари', fontsize=16)\n",
        "\n",
        "ax.set_zlim(0., z_mesh.max())\n",
        "ax.view_init(elev = 20, azim = 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3uBVyGKDZgn"
      },
      "source": [
        "__Кривые обучения__ — это графическое представление зависимости меры (показателя) качества обучения (по вертикальной оси) от определенного показателя модели обучения (по горизонтальной оси). Будем визуализировать в качестве качества модели показатели RMSE для части обучающей выборки и тестовой выборки в зависимости от количества точек в обучающей выборке.\n",
        "\n",
        "Для разбиения на обучающую и тестовую выборки можно использовать функцию `train_test_split`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4CS6Gl6DZgn"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X, y, test_ratio=0.2, seed=None):\n",
        "    \"\"\"возвращает X_train, X_test, y_train, y_test\"\"\"\n",
        "    assert X.shape[0] == y.shape[0], \\\n",
        "        \"Размер X должен быть равен размеру y\"\n",
        "    assert 0.0 <= test_ratio <= 1.0, \\\n",
        "        \"Неверное значение test_ratio\"\n",
        "\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    shuffled_indexes = np.random.permutation(len(X))\n",
        "\n",
        "    test_size = int(len(X) * test_ratio)\n",
        "    test_indexes = shuffled_indexes[:test_size]\n",
        "    train_indexes = shuffled_indexes[test_size:]\n",
        "\n",
        "    X_train = X[train_indexes]\n",
        "    y_train = y[train_indexes]\n",
        "\n",
        "    X_test = X[test_indexes]\n",
        "    y_test = y[test_indexes]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd4QB7zHDZgn"
      },
      "source": [
        "Разобьем массивы данных `X` и `y` на обучающие и тестовые данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8GxiCxEDZgo"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.3)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO7FkO6qDZgo"
      },
      "outputs": [],
      "source": [
        "def my_rmse(y_test, y_predict):\n",
        "    return np.sqrt(np.sum((y_predict - y_test)**2) / len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KClVgxgcDZgo"
      },
      "outputs": [],
      "source": [
        "train_score = []\n",
        "test_score = []\n",
        "for i in range(11, 381, 10):\n",
        "    large_model = tf.keras.Sequential([\n",
        "        feature_normalizer,\n",
        "#        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "#        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "#        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=1)\n",
        "    ])\n",
        "    large_model.compile(loss='mse')\n",
        "    large_model.fit(X_train[:i], y_train[:i], epochs=50, verbose=0)\n",
        "\n",
        "    y_train_predict = large_model.predict(X_train[:i])\n",
        "    train_score.append(my_rmse(y_train[:i], y_train_predict))\n",
        "    \n",
        "    y_test_predict = large_model.predict(X_test)\n",
        "    test_score.append(my_rmse(y_test, y_test_predict))\n",
        "    print('-->', i, ' done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ji6cTYFDZgo"
      },
      "outputs": [],
      "source": [
        "plt.plot([i for i in range(11, len(X_train), 10)], \n",
        "                               train_score, label=\"train\")\n",
        "plt.plot([i for i in range(11, len(X_train), 10)], \n",
        "                               test_score, label=\"test\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvrOTmXEBUVS"
      },
      "source": [
        "#### Задание (10 баллов)\n",
        "\n",
        "Для закрепленного за Вами варианта лабораторной работы:\n",
        "\n",
        "1.\tЗагрузите заданный в индивидуальном задании набор данных из Tensorflow Datasets, включая указанные в задании независимый признак и зависимый признак (отклик).\n",
        "\n",
        "2.\tРешите задачу полиномиальной регрессии для степени полинома, указанной в индивидуальном задании, при помощи нейронной сети с одним нейроном и оцените качество полученной модели по показателю, указанному в индивидуальном задании.   \n",
        "\n",
        "3. Постройте кривые обучения с зависимостью от количества эпох.\n",
        "\n",
        "4.  Визуализируйте точки набора данных на плоскости в виде диаграммы рассеяния (ось X – независимый признак, ось Y – зависимый признак), а также линию регрессии (другим цветом), подписывая оси и рисунок.  \n",
        "\n",
        "5. Определите в исходном наборе данных признак (отличный от независимого и зависимого признаков), принимающий непрерывные значения и имеющий свойства, указанные в индивидуальном задании. \n",
        "\n",
        "6. Визуализируйте этот признак в соответствии с индивидуальным заданием. \n",
        "\n",
        "7. Сформируйте набор входных из двух признаков набора данных (независимый признак и определенный признак), создайте и адаптируйте нормализующий слой Tensorflow для двух признаков. \n",
        "\n",
        "8.\tИспользуя созданный нормализующий слой, постройте нейронную сеть (нелинейный регресор) с количеством скрытых слоев, количеством нейронов и функцией активации, указанными в индивидуальном задании, и одним нейроном в выходном слое и обучите ее на наборе данных из двух признаков и отклика. \n",
        "\n",
        "9. Визуализируйте набор данных в виде точечного графика и прогноз нейронной сети в виде поверхности в трехмерном пространстве.\n",
        "\n",
        "10. Разбейте набор данных из двух признаков и отклика на обучающую и тестовую выборки и постройте кривые обучения для заданного показателя качества в зависимости от количества точек в обучающей выборке, подписывая оси и рисунок и создавая легенду. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ul5EHxsDZgp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab_3_fin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}